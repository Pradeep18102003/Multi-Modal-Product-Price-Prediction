{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13348797,"sourceType":"datasetVersion","datasetId":8465669}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# --- Imports ---\nimport pandas as pd\nimport numpy as np\nimport os\nimport shutil # <-- Import shutil for deleting directories\nfrom pathlib import Path\nimport urllib.request\nimport multiprocessing\nfrom functools import partial\nfrom tqdm import tqdm\nimport time\n\n# --- Model & Feature Extraction Imports ---\nimport tensorflow as tf\nfrom tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input\nfrom tensorflow.keras.preprocessing import image\nfrom sentence_transformers import SentenceTransformer\nimport lightgbm as lgb\nfrom sklearn.model_selection import train_test_split\n\n# --- Helper Function: SMAPE Metric ---\ndef smape(y_true, y_pred):\n    numerator = np.abs(y_pred - y_true)\n    denominator = (np.abs(y_true) + np.abs(y_pred)) / 2\n    epsilon = 1e-8\n    return np.mean(numerator / (denominator + epsilon)) * 100\n\n# --- Helper Function: Image Downloader ---\ndef download_image(image_link, savefolder):\n    if isinstance(image_link, str):\n        filename = Path(image_link).name\n        image_save_path = os.path.join(savefolder, filename)\n        if not os.path.exists(image_save_path):\n            try:\n                urllib.request.urlretrieve(image_link, image_save_path)\n            except Exception as ex:\n                pass\n    return\n\ndef download_images_parallel(image_links, download_folder):\n    if not os.path.exists(download_folder):\n        os.makedirs(download_folder)\n    download_image_partial = partial(download_image, savefolder=download_folder)\n    with multiprocessing.Pool(processes=multiprocessing.cpu_count()) as pool:\n        list(tqdm(pool.imap(download_image_partial, image_links), total=len(image_links)))\n\nprint(\"Setup Complete. All functions and libraries are loaded.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T15:51:52.091666Z","iopub.execute_input":"2025-10-13T15:51:52.091953Z","iopub.status.idle":"2025-10-13T15:52:04.387798Z","shell.execute_reply.started":"2025-10-13T15:51:52.091932Z","shell.execute_reply":"2025-10-13T15:52:04.387073Z"}},"outputs":[{"name":"stdout","text":"Setup Complete. All functions and libraries are loaded.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import sentence_transformers, transformers, huggingface_hub\nprint(\"sentence-transformers:\", sentence_transformers.__version__)\nprint(\"transformers:\", transformers.__version__)\nprint(\"huggingface-hub:\", huggingface_hub.__version__)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T15:52:04.389009Z","iopub.execute_input":"2025-10-13T15:52:04.389636Z","iopub.status.idle":"2025-10-13T15:52:04.394188Z","shell.execute_reply.started":"2025-10-13T15:52:04.389614Z","shell.execute_reply":"2025-10-13T15:52:04.393360Z"}},"outputs":[{"name":"stdout","text":"sentence-transformers: 2.7.0\ntransformers: 4.41.2\nhuggingface-hub: 0.23.2\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import torch\n\n# Check for GPU availability and set the device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f'Using device: {device}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T15:52:04.395736Z","iopub.execute_input":"2025-10-13T15:52:04.395949Z","iopub.status.idle":"2025-10-13T15:52:04.470363Z","shell.execute_reply.started":"2025-10-13T15:52:04.395933Z","shell.execute_reply":"2025-10-13T15:52:04.469625Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"#!pip install tensorflow==2.16.1 --upgrade","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T15:52:04.471800Z","iopub.execute_input":"2025-10-13T15:52:04.472075Z","iopub.status.idle":"2025-10-13T15:52:04.485193Z","shell.execute_reply.started":"2025-10-13T15:52:04.472056Z","shell.execute_reply":"2025-10-13T15:52:04.484517Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"#!pip install -U sentence-transformers==2.7.0 transformers==4.41.2 huggingface-hub==0.23.2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T15:52:04.485781Z","iopub.execute_input":"2025-10-13T15:52:04.486017Z","iopub.status.idle":"2025-10-13T15:52:04.496902Z","shell.execute_reply.started":"2025-10-13T15:52:04.486002Z","shell.execute_reply":"2025-10-13T15:52:04.496120Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# --- 1. Load Data ---\nprint(\"--- Loading Full Data ---\")\ntrain_df = pd.read_csv('/kaggle/input/amazon-ml/train.csv')\ntest_df = pd.read_csv('/kaggle/input/amazon-ml/test.csv')\n\n\n# Apply log transformation to the price\ntrain_df['log_price'] = np.log1p(train_df['price'])\nprint(\"DataFrames loaded successfully.\")\nprint(f\"Using {len(train_df)} training samples and {len(test_df)} testing samples.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T15:52:04.659918Z","iopub.execute_input":"2025-10-13T15:52:04.660109Z","iopub.status.idle":"2025-10-13T15:52:07.729917Z","shell.execute_reply.started":"2025-10-13T15:52:04.660094Z","shell.execute_reply":"2025-10-13T15:52:07.729024Z"}},"outputs":[{"name":"stdout","text":"--- Loading Full Data ---\nDataFrames loaded successfully.\nUsing 75000 training samples and 75000 testing samples.\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"train_df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T15:52:07.731278Z","iopub.execute_input":"2025-10-13T15:52:07.731510Z","iopub.status.idle":"2025-10-13T15:52:07.745889Z","shell.execute_reply.started":"2025-10-13T15:52:07.731492Z","shell.execute_reply":"2025-10-13T15:52:07.745127Z"}},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"   sample_id                                    catalog_content  \\\n0      33127  Item Name: La Victoria Green Taco Sauce Mild, ...   \n1     198967  Item Name: Salerno Cookies, The Original Butte...   \n2     261251  Item Name: Bear Creek Hearty Soup Bowl, Creamy...   \n3      55858  Item Name: Judee’s Blue Cheese Powder 11.25 oz...   \n4     292686  Item Name: kedem Sherry Cooking Wine, 12.7 Oun...   \n\n                                          image_link  price  log_price  \n0  https://m.media-amazon.com/images/I/51mo8htwTH...   4.89   1.773256  \n1  https://m.media-amazon.com/images/I/71YtriIHAA...  13.12   2.647592  \n2  https://m.media-amazon.com/images/I/51+PFEe-w-...   1.97   1.088562  \n3  https://m.media-amazon.com/images/I/41mu0HAToD...  30.34   3.444895  \n4  https://m.media-amazon.com/images/I/41sA037+Qv...  66.49   4.211979  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sample_id</th>\n      <th>catalog_content</th>\n      <th>image_link</th>\n      <th>price</th>\n      <th>log_price</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>33127</td>\n      <td>Item Name: La Victoria Green Taco Sauce Mild, ...</td>\n      <td>https://m.media-amazon.com/images/I/51mo8htwTH...</td>\n      <td>4.89</td>\n      <td>1.773256</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>198967</td>\n      <td>Item Name: Salerno Cookies, The Original Butte...</td>\n      <td>https://m.media-amazon.com/images/I/71YtriIHAA...</td>\n      <td>13.12</td>\n      <td>2.647592</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>261251</td>\n      <td>Item Name: Bear Creek Hearty Soup Bowl, Creamy...</td>\n      <td>https://m.media-amazon.com/images/I/51+PFEe-w-...</td>\n      <td>1.97</td>\n      <td>1.088562</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>55858</td>\n      <td>Item Name: Judee’s Blue Cheese Powder 11.25 oz...</td>\n      <td>https://m.media-amazon.com/images/I/41mu0HAToD...</td>\n      <td>30.34</td>\n      <td>3.444895</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>292686</td>\n      <td>Item Name: kedem Sherry Cooking Wine, 12.7 Oun...</td>\n      <td>https://m.media-amazon.com/images/I/41sA037+Qv...</td>\n      <td>66.49</td>\n      <td>4.211979</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"# --- 2. Process Training Images ---\nTRAIN_IMAGE_DIR = 'dataset/images/train'\nTRAIN_IMG_EMBEDDINGS_FILE = 'train_image_embeddings.npy'\n\nif os.path.exists(TRAIN_IMG_EMBEDDINGS_FILE):\n    print(\"✅ Training image embeddings already exist. Loading from file.\")\n    train_image_embeddings = np.load(TRAIN_IMG_EMBEDDINGS_FILE)\nelse:\n    print(f\"--- Downloading {len(train_df)} Training Images ---\")\n    download_images_parallel(train_df['image_link'].tolist(), TRAIN_IMAGE_DIR)\n    \n    print(\"\\n--- Extracting Training Image Features ---\")\n    image_model = ResNet50(weights='imagenet', include_top=False, pooling='avg')\n\n    \n    \n    # MODIFIED Code\n    def extract_image_features_batched(image_paths, batch_size=64):\n        all_features = []\n        num_images = len(image_paths)\n        for i in tqdm(range(0, num_images, batch_size)):\n            batch_paths = image_paths[i:i + batch_size]\n            batch_images = []\n            # Keep track of which images failed to load\n            valid_indices = []\n            \n            for idx, path in enumerate(batch_paths):\n                try:\n                    img = image.load_img(path, target_size=(224, 224))\n                    img_array = image.img_to_array(img)\n                    batch_images.append(img_array)\n                    valid_indices.append(True)\n                except Exception:\n                    valid_indices.append(False) # Mark this image as failed\n    \n            if not batch_images: # If the whole batch failed\n                all_features.extend([np.zeros(2048) for _ in batch_paths])\n                continue\n                \n            # Preprocess the entire batch at once\n            batch_images_np = np.array(batch_images)\n            preprocessed_batch = preprocess_input(batch_images_np)\n            \n            # Predict on the entire batch\n            feature_vectors = image_model.predict(preprocessed_batch, verbose=0)\n            \n            # Place results back correctly, filling failures with zeros\n            features_with_zeros = []\n            feature_iter = iter(feature_vectors)\n            for is_valid in valid_indices:\n                if is_valid:\n                    features_with_zeros.append(next(feature_iter).flatten())\n                else:\n                    features_with_zeros.append(np.zeros(2048))\n            all_features.extend(features_with_zeros)\n    \n        return np.array(all_features)\n    \n    # --- Then, call the new function ---\n    train_image_paths = [os.path.join(TRAIN_IMAGE_DIR, Path(link).name) for link in train_df['image_link']]\n    train_image_embeddings = extract_image_features_batched(train_image_paths)\n    # (and do the same for the test images in the next cell)\n    \n    print(\"\\n--- Saving Training Image Embeddings ---\")\n    np.save(TRAIN_IMG_EMBEDDINGS_FILE, train_image_embeddings)\n    \n    print(\"\\n--- Deleting Training Images to Free Space ---\")\n    shutil.rmtree(TRAIN_IMAGE_DIR)\n    print(\"✅ Training images deleted.\")\n\nprint(f\"Train image embeddings ready. Shape: {train_image_embeddings.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T15:52:07.746789Z","iopub.execute_input":"2025-10-13T15:52:07.747048Z","iopub.status.idle":"2025-10-13T15:52:13.065131Z","shell.execute_reply.started":"2025-10-13T15:52:07.747026Z","shell.execute_reply":"2025-10-13T15:52:13.064249Z"}},"outputs":[{"name":"stdout","text":"✅ Training image embeddings already exist. Loading from file.\nTrain image embeddings ready. Shape: (75000, 2048)\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# --- 3. Process Test Images ---\nTEST_IMAGE_DIR = 'dataset/images/test'\nTEST_IMG_EMBEDDINGS_FILE = 'test_image_embeddings.npy'\n\nif os.path.exists(TEST_IMG_EMBEDDINGS_FILE):\n    print(\"✅ Test image embeddings already exist. Loading from file.\")\n    test_image_embeddings = np.load(TEST_IMG_EMBEDDINGS_FILE)\nelse:\n    print(f\"--- Downloading {len(test_df)} Test Images ---\")\n    download_images_parallel(test_df['image_link'].tolist(), TEST_IMAGE_DIR)\n    \n    print(\"\\n--- Extracting Test Image Features ---\")\n    # We can reuse the image_model if the previous cell was run in the same session\n    try:\n        image_model\n    except NameError:\n        image_model = ResNet50(weights='imagenet', include_top=False, pooling='avg')\n\n    # Re-using the feature extraction function defined in the previous cell\n    test_image_paths = [os.path.join(TEST_IMAGE_DIR, Path(link).name) for link in test_df['image_link']]\n    test_image_embeddings = extract_image_features_batched(test_image_paths)\n    \n    print(\"\\n--- Saving Test Image Embeddings ---\")\n    np.save(TEST_IMG_EMBEDDINGS_FILE, test_image_embeddings)\n    \n    print(\"\\n--- Deleting Test Images to Free Space ---\")\n    shutil.rmtree(TEST_IMAGE_DIR)\n    print(\"✅ Test images deleted.\")\n\nprint(f\"Test image embeddings ready. Shape: {test_image_embeddings.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T15:52:13.066788Z","iopub.execute_input":"2025-10-13T15:52:13.067040Z","iopub.status.idle":"2025-10-13T15:52:13.515012Z","shell.execute_reply.started":"2025-10-13T15:52:13.067022Z","shell.execute_reply":"2025-10-13T15:52:13.514098Z"}},"outputs":[{"name":"stdout","text":"✅ Test image embeddings already exist. Loading from file.\nTest image embeddings ready. Shape: (75000, 2048)\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# --- 4. Text Feature Extraction ---\nTRAIN_TXT_EMBEDDINGS_FILE = 'train_text_embeddings.npy'\nTEST_TXT_EMBEDDINGS_FILE = 'test_text_embeddings.npy'\n\nif os.path.exists(TRAIN_TXT_EMBEDDINGS_FILE) and os.path.exists(TEST_TXT_EMBEDDINGS_FILE):\n    print(\"--- Loading text embeddings from saved files ---\")\n    train_text_embeddings = np.load(TRAIN_TXT_EMBEDDINGS_FILE)\n    test_text_embeddings = np.load(TEST_TXT_EMBEDDINGS_FILE)\n    print(\"✅ Text embeddings loaded successfully.\")\nelse:\n    print(\"--- Generating and saving text embeddings ---\")\n    # MODIFIED Code\n    # Explicitly tell the model to use the 'cuda' (GPU) device\n    text_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2', trust_remote_code=True, device='cuda')\n    \n    # Increase the batch size to process more data at once on the GPU\n    train_text_embeddings = text_model.encode(\n        train_df['catalog_content'].tolist(),\n        show_progress_bar=True,\n        batch_size=256  # Adjust batch size based on GPU memory\n    )\n    test_text_embeddings = text_model.encode(\n        test_df['catalog_content'].tolist(),\n        show_progress_bar=True,\n        batch_size=256\n    )\n    \n    np.save(TRAIN_TXT_EMBEDDINGS_FILE, train_text_embeddings)\n    np.save(TEST_TXT_EMBEDDINGS_FILE, test_text_embeddings)\n    print(\"✅ Text embeddings generated and saved.\")\n\nprint(f\"Train text embeddings shape: {train_text_embeddings.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T15:52:13.515932Z","iopub.execute_input":"2025-10-13T15:52:13.516494Z","iopub.status.idle":"2025-10-13T15:52:13.607467Z","shell.execute_reply.started":"2025-10-13T15:52:13.516463Z","shell.execute_reply":"2025-10-13T15:52:13.606775Z"}},"outputs":[{"name":"stdout","text":"--- Loading text embeddings from saved files ---\n✅ Text embeddings loaded successfully.\nTrain text embeddings shape: (75000, 384)\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- 5. Combine Features & Train Model ---\nprint(\"\\n--- Combining Features and Training Model ---\")\ncombined_train_features = np.hstack([train_text_embeddings, train_image_embeddings])\n\nX = combined_train_features\ny = train_df['log_price']\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\ny_val_original = np.expm1(y_val)\n\n# --- You can change model parameters here and rerun ---\nlgbm = lgb.LGBMRegressor(random_state=42, n_estimators=200, learning_rate=0.05)\nlgbm.fit(X_train, y_train)\n\n# --- Evaluate the new model ---\nprint(\"\\n--- Evaluating the new model ---\")\nlog_val_preds = lgbm.predict(X_val)\nval_preds = np.expm1(log_val_preds)\nvalidation_score = smape(y_val_original, val_preds)\nprint(f\"✅ Validation SMAPE Score (Multi-modal): {validation_score:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T15:52:13.608183Z","iopub.execute_input":"2025-10-13T15:52:13.608446Z","iopub.status.idle":"2025-10-13T15:54:41.832925Z","shell.execute_reply.started":"2025-10-13T15:52:13.608427Z","shell.execute_reply":"2025-10-13T15:54:41.832094Z"}},"outputs":[{"name":"stdout","text":"\n--- Combining Features and Training Model ---\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 4.652498 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 620160\n[LightGBM] [Info] Number of data points in the train set: 60000, number of used features: 2432\n[LightGBM] [Info] Start training from score 2.740904\n\n--- Evaluating the new model ---\n✅ Validation SMAPE Score (Multi-modal): 60.4032\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"### **Some Feature Engineering**","metadata":{}},{"cell_type":"code","source":"import re\nimport numpy as np\nimport pandas as pd\n\n# ===================================================================\n# 1. DEFINE ALL FEATURE ENGINEERING FUNCTIONS\n# ===================================================================\n\n# --- For Quantity (Weight/Volume) ---\nUNIT_CONVERSIONS = {\n    # Weight\n    'kg': 1000, 'kilogram': 1000, 'kgs': 1000,\n    'g': 1, 'gm': 1, 'gram': 1, 'gms': 1,\n    'mg': 0.001, 'milligram': 0.001,\n    'lb': 453.592, 'pound': 453.592, 'lbs': 453.592,\n    'oz': 28.35, 'ounce': 28.35,\n    # Volume\n    'l': 1000, 'liter': 1000, 'liters': 1000,\n    'ml': 1, 'milliliter': 1,\n    'floz': 29.5735, 'fluid ounce': 29.5735\n}\n\ndef extract_quantity(text):\n    text = str(text).lower()\n    pattern = r\"(\\d+\\.?\\d*)\\s?(\" + \"|\".join(UNIT_CONVERSIONS.keys()) + r\")\\b\"\n    match = re.search(pattern, text)\n    if match:\n        value = float(match.group(1))\n        unit = match.group(2)\n        return value * UNIT_CONVERSIONS[unit]\n    return np.nan\n\n# --- For Pack Count ---\ndef extract_pack_count(text):\n    text = str(text).lower()\n    match = re.search(r\"(?:pack|set)\\s+of\\s+(\\d+)\", text)\n    if match:\n        return int(match.group(1))\n    match = re.search(r\"(\\d+)\\s*(?:-|pack|count|ct)\\b\", text)\n    if match:\n        return int(match.group(1))\n    return 1 # Default to 1 if no pack info found\n\n# --- For Brand Name ---\ndef extract_brand(text):\n    try:\n        return str(text).split()[2].lower()\n    except IndexError:\n        return \"unknown\"\n\n# ===================================================================","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T16:54:20.617003Z","iopub.execute_input":"2025-10-13T16:54:20.617736Z","iopub.status.idle":"2025-10-13T16:54:20.625116Z","shell.execute_reply.started":"2025-10-13T16:54:20.617713Z","shell.execute_reply":"2025-10-13T16:54:20.624324Z"}},"outputs":[],"execution_count":45},{"cell_type":"code","source":"# 2. APPLY FUNCTIONS TO DATAFRAMES\n# ===================================================================\nprint(\"--- Starting Feature Engineering ---\")\n\n# We'll operate on both DataFrames at once\nfor df in [train_df, test_df]:\n    print(f\"Processing DataFrame of shape: {df.shape}\")\n    \n    # Extract numerical features\n    df['quantity_std'] = df['catalog_content'].apply(extract_quantity)\n    df['pack_count'] = df['catalog_content'].apply(extract_pack_count)\n    df['text_length'] = df['catalog_content'].str.len()\n    \n    # Extract keyword flag\n    df['is_organic'] = df['catalog_content'].str.contains('organic', case=False, regex=False).astype(int)\n    \n    # Extract brand name\n    df['brand'] = df['catalog_content'].apply(extract_brand)\n\nprint(\"\\n--- Raw Feature Extraction Complete ---\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T16:54:21.017773Z","iopub.execute_input":"2025-10-13T16:54:21.018039Z","iopub.status.idle":"2025-10-13T16:54:28.250961Z","shell.execute_reply.started":"2025-10-13T16:54:21.018020Z","shell.execute_reply":"2025-10-13T16:54:28.250282Z"}},"outputs":[{"name":"stdout","text":"--- Starting Feature Engineering ---\nProcessing DataFrame of shape: (75000, 10)\nProcessing DataFrame of shape: (75000, 8)\n\n--- Raw Feature Extraction Complete ---\n","output_type":"stream"}],"execution_count":46},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T16:54:28.252185Z","iopub.execute_input":"2025-10-13T16:54:28.252504Z","iopub.status.idle":"2025-10-13T16:54:28.262046Z","shell.execute_reply.started":"2025-10-13T16:54:28.252485Z","shell.execute_reply":"2025-10-13T16:54:28.261341Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1458: RuntimeWarning: invalid value encountered in greater\n  has_large_values = (abs_vals > 1e6).any()\n/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in less\n  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in greater\n  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n","output_type":"stream"},{"execution_count":47,"output_type":"execute_result","data":{"text/plain":"   sample_id                                    catalog_content  \\\n0     100179  Item Name: Rani 14-Spice Eshamaya's Mango Chut...   \n1     245611  Item Name: Natural MILK TEA Flavoring extract ...   \n2     146263  Item Name: Honey Filled Hard Candy - Bulk Pack...   \n3      95658  Item Name: Vlasic Snack'mm's Kosher Dill 16 Oz...   \n4      36806  Item Name: McCormick Culinary Vanilla Extract,...   \n\n                                          image_link  quantity_std  \\\n0  https://m.media-amazon.com/images/I/71hoAn78AW...       297.675   \n1  https://m.media-amazon.com/images/I/61ex8NHCIj...        56.700   \n2  https://m.media-amazon.com/images/I/61KCM61J8e...           NaN   \n3  https://m.media-amazon.com/images/I/51Ex6uOH7y...       453.600   \n4  https://m.media-amazon.com/images/I/71QYlrOMoS...       946.352   \n\n   pack_count  text_length  is_organic      brand  \n0          14         1274           0       rani  \n1           1         1720           0    natural  \n2           2          769           0      honey  \n3           2           82           0     vlasic  \n4          32         1491           0  mccormick  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sample_id</th>\n      <th>catalog_content</th>\n      <th>image_link</th>\n      <th>quantity_std</th>\n      <th>pack_count</th>\n      <th>text_length</th>\n      <th>is_organic</th>\n      <th>brand</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>100179</td>\n      <td>Item Name: Rani 14-Spice Eshamaya's Mango Chut...</td>\n      <td>https://m.media-amazon.com/images/I/71hoAn78AW...</td>\n      <td>297.675</td>\n      <td>14</td>\n      <td>1274</td>\n      <td>0</td>\n      <td>rani</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>245611</td>\n      <td>Item Name: Natural MILK TEA Flavoring extract ...</td>\n      <td>https://m.media-amazon.com/images/I/61ex8NHCIj...</td>\n      <td>56.700</td>\n      <td>1</td>\n      <td>1720</td>\n      <td>0</td>\n      <td>natural</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>146263</td>\n      <td>Item Name: Honey Filled Hard Candy - Bulk Pack...</td>\n      <td>https://m.media-amazon.com/images/I/61KCM61J8e...</td>\n      <td>NaN</td>\n      <td>2</td>\n      <td>769</td>\n      <td>0</td>\n      <td>honey</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>95658</td>\n      <td>Item Name: Vlasic Snack'mm's Kosher Dill 16 Oz...</td>\n      <td>https://m.media-amazon.com/images/I/51Ex6uOH7y...</td>\n      <td>453.600</td>\n      <td>2</td>\n      <td>82</td>\n      <td>0</td>\n      <td>vlasic</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>36806</td>\n      <td>Item Name: McCormick Culinary Vanilla Extract,...</td>\n      <td>https://m.media-amazon.com/images/I/71QYlrOMoS...</td>\n      <td>946.352</td>\n      <td>32</td>\n      <td>1491</td>\n      <td>0</td>\n      <td>mccormick</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":47},{"cell_type":"code","source":"df.isnull().sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T16:54:38.156618Z","iopub.execute_input":"2025-10-13T16:54:38.157341Z","iopub.status.idle":"2025-10-13T16:54:38.182551Z","shell.execute_reply.started":"2025-10-13T16:54:38.157319Z","shell.execute_reply":"2025-10-13T16:54:38.181998Z"}},"outputs":[{"execution_count":48,"output_type":"execute_result","data":{"text/plain":"sample_id              0\ncatalog_content        0\nimage_link             0\nquantity_std       20637\npack_count             0\ntext_length            0\nis_organic             0\nbrand                  0\ndtype: int64"},"metadata":{}}],"execution_count":48},{"cell_type":"code","source":"# 3. HANDLE MISSING VALUES & ENCODE CATEGORICALS\n# ===================================================================\n\n# --- Handle Missing Numerical Values ---\n# Calculate median from the training set ONLY\nmedian_quantity = train_df['quantity_std'].median()\nmedian_length = train_df['text_length'].median()\n\n# Fill NaNs in both train and test sets using the training median\nfor df in [train_df, test_df]:\n    df['quantity_std'].fillna(median_quantity, inplace=True)\n    df['text_length'].fillna(median_length, inplace=True)\n\nprint(f\"Missing quantities filled with median value: {median_quantity}\")\n\n# --- Encode Brand as a Categorical Feature ---\n# Get all unique brands across both datasets to ensure consistency\nall_brands = pd.concat([train_df['brand'], test_df['brand']]).unique()\n\n# Convert the 'brand' column into a pandas Categorical type\ntrain_df['brand'] = pd.Categorical(train_df['brand'], categories=all_brands)\ntest_df['brand'] = pd.Categorical(test_df['brand'], categories=all_brands)\n\nprint(\"Brand column successfully encoded.\")\n\n# ===================================================================","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T16:54:40.214955Z","iopub.execute_input":"2025-10-13T16:54:40.215551Z","iopub.status.idle":"2025-10-13T16:54:40.260307Z","shell.execute_reply.started":"2025-10-13T16:54:40.215528Z","shell.execute_reply":"2025-10-13T16:54:40.259536Z"}},"outputs":[{"name":"stdout","text":"Missing quantities filled with median value: 283.5\nBrand column successfully encoded.\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_28340/3420886458.py:11: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  df['quantity_std'].fillna(median_quantity, inplace=True)\n/tmp/ipykernel_28340/3420886458.py:12: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  df['text_length'].fillna(median_length, inplace=True)\n","output_type":"stream"}],"execution_count":49},{"cell_type":"code","source":"df.isnull().sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T16:54:43.365660Z","iopub.execute_input":"2025-10-13T16:54:43.366382Z","iopub.status.idle":"2025-10-13T16:54:43.387445Z","shell.execute_reply.started":"2025-10-13T16:54:43.366359Z","shell.execute_reply":"2025-10-13T16:54:43.386765Z"}},"outputs":[{"execution_count":50,"output_type":"execute_result","data":{"text/plain":"sample_id          0\ncatalog_content    0\nimage_link         0\nquantity_std       0\npack_count         0\ntext_length        0\nis_organic         0\nbrand              0\ndtype: int64"},"metadata":{}}],"execution_count":50},{"cell_type":"code","source":"# 4. PREPARE FINAL FEATURE SET\n# ===================================================================\n\n# Select all the engineered feature columns we want to use\nengineered_feature_cols = ['quantity_std', 'pack_count', 'text_length', 'is_organic', 'brand']\n\n# Create the final feature DataFrames for the model\ntrain_engineered_features_df = train_df[engineered_feature_cols]\ntest_engineered_features_df = test_df[engineered_feature_cols]\n\nprint(\"\\n--- Feature Engineering Complete! ---\")\nprint(\"Final engineered features ready for model training. Example:\")\nprint(train_engineered_features_df.head())\nprint(\"\\nData types of new features:\")\nprint(train_engineered_features_df.info())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T16:54:45.399679Z","iopub.execute_input":"2025-10-13T16:54:45.400317Z","iopub.status.idle":"2025-10-13T16:54:45.424936Z","shell.execute_reply.started":"2025-10-13T16:54:45.400294Z","shell.execute_reply":"2025-10-13T16:54:45.424350Z"}},"outputs":[{"name":"stdout","text":"\n--- Feature Engineering Complete! ---\nFinal engineered features ready for model training. Example:\n   quantity_std  pack_count  text_length  is_organic    brand\n0      340.2000           6           91           0       la\n1      226.8000           4          511           0  salerno\n2       53.8650           6          328           0     bear\n3      318.9375           1         1318           0  judee’s\n4      360.0450           1          155           0    kedem\n\nData types of new features:\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 75000 entries, 0 to 74999\nData columns (total 5 columns):\n #   Column        Non-Null Count  Dtype   \n---  ------        --------------  -----   \n 0   quantity_std  75000 non-null  float64 \n 1   pack_count    75000 non-null  int64   \n 2   text_length   75000 non-null  int64   \n 3   is_organic    75000 non-null  int64   \n 4   brand         75000 non-null  category\ndtypes: category(1), float64(1), int64(3)\nmemory usage: 3.0 MB\nNone\n","output_type":"stream"}],"execution_count":51},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport lightgbm as lgb\nfrom sklearn.model_selection import train_test_split\n\nprint(\"--- Combining all feature sets with unique column names ---\")\n\n# Convert numpy embedding arrays to pandas DataFrames with unique prefixes\ntrain_text_df = pd.DataFrame(train_text_embeddings, index=train_df.index, columns=[f'txt_{i}' for i in range(train_text_embeddings.shape[1])])\ntrain_image_df = pd.DataFrame(train_image_embeddings, index=train_df.index, columns=[f'img_{i}' for i in range(train_image_embeddings.shape[1])])\n\ntest_text_df = pd.DataFrame(test_text_embeddings, index=test_df.index, columns=[f'txt_{i}' for i in range(test_text_embeddings.shape[1])])\ntest_image_df = pd.DataFrame(test_image_embeddings, index=test_df.index, columns=[f'img_{i}' for i in range(test_image_embeddings.shape[1])])\n\n# Concatenate all features horizontally (axis=1)\nX = pd.concat([\n    train_text_df,\n    train_image_df,\n    train_engineered_features_df\n], axis=1)\n\nX_test = pd.concat([\n    test_text_df,\n    test_image_df,\n    test_engineered_features_df\n], axis=1)\n\ny = train_df['log_price']\n\nprint(f\"Final training feature shape: {X.shape}\")\nprint(f\"Final test feature shape: {X_test.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T16:54:46.538599Z","iopub.execute_input":"2025-10-13T16:54:46.539146Z","iopub.status.idle":"2025-10-13T16:54:52.184753Z","shell.execute_reply.started":"2025-10-13T16:54:46.539124Z","shell.execute_reply":"2025-10-13T16:54:52.183913Z"}},"outputs":[{"name":"stdout","text":"--- Combining all feature sets with unique column names ---\nFinal training feature shape: (75000, 2437)\nFinal test feature shape: (75000, 2437)\n","output_type":"stream"}],"execution_count":52},{"cell_type":"code","source":"X.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T16:54:55.207011Z","iopub.execute_input":"2025-10-13T16:54:55.207837Z","iopub.status.idle":"2025-10-13T16:54:55.213129Z","shell.execute_reply.started":"2025-10-13T16:54:55.207800Z","shell.execute_reply":"2025-10-13T16:54:55.212245Z"}},"outputs":[{"execution_count":53,"output_type":"execute_result","data":{"text/plain":"(75000, 2437)"},"metadata":{}}],"execution_count":53},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport time\n\n# Import all the models we want to test\nimport lightgbm as lgb\nimport xgboost as xgb\nimport catboost as cb\n\nfrom sklearn.model_selection import train_test_split\n\n# ===================================================================\n# 1. SETUP THE BENCHMARK\n# ===================================================================\n\n# Split data for validation, which we'll use for all models\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\ny_val_original = np.expm1(y_val) # For SMAPE calculation\n\n# Define the models to test in a dictionary\nmodels = {\n    \"LightGBM\": lgb.LGBMRegressor(random_state=42, n_estimators=1000),\n    \"XGBoost\": xgb.XGBRegressor(random_state=42, n_estimators=1000, n_jobs=-1, early_stopping_rounds=100),\n    \"CatBoost\": cb.CatBoostRegressor(random_state=42, n_estimators=1000, verbose=0, early_stopping_rounds=100)\n}\n\nresults = {}\nbest_iterations = {}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T16:55:26.223497Z","iopub.execute_input":"2025-10-13T16:55:26.224291Z","iopub.status.idle":"2025-10-13T16:55:27.071070Z","shell.execute_reply.started":"2025-10-13T16:55:26.224264Z","shell.execute_reply":"2025-10-13T16:55:27.070463Z"}},"outputs":[],"execution_count":54},{"cell_type":"code","source":"# ===================================================================\n# 2. RUN THE BENCHMARKING LOOP\n# ===================================================================\nprint(\"--- Starting Model Benchmarking ---\")\n\nfor name, model in models.items():\n    start_time = time.time()\n    print(f\"\\n--- Training {name} ---\")\n\n    # Train each model with its specific parameters\n    if name == \"LightGBM\":\n        model.fit(X_train, y_train, eval_set=[(X_val, y_val)],\n                  callbacks=[lgb.early_stopping(100, verbose=False)],\n                  categorical_feature=['brand'])\n        best_iterations[name] = model.best_iteration_\n    elif name == \"XGBoost\":\n        # XGBoost needs category columns to be of 'category' dtype, which we already did\n        X_train_xgb = X_train.copy()\n        X_val_xgb = X_val.copy()\n        X_train_xgb['brand'] = X_train_xgb['brand'].cat.codes\n        X_val_xgb['brand'] = X_val_xgb['brand'].cat.codes\n        model.fit(X_train_xgb, y_train, eval_set=[(X_val_xgb, y_val)])\n        best_iterations[name] = model.best_iteration\n    elif name == \"CatBoost\":\n        model.fit(X_train, y_train, eval_set=[(X_val, y_val)],\n                  cat_features=['brand'])\n        best_iterations[name] = model.best_iteration_\n    \n    # Make predictions and evaluate\n    log_val_preds = model.predict(X_val)\n    val_preds = np.expm1(log_val_preds)\n    score = smape(y_val_original, val_preds)\n    \n    # Store results\n    execution_time = time.time() - start_time\n    results[name] = {'score': score, 'time': execution_time}\n    print(f\"✅ {name} Validation SMAPE: {score:.4f} (trained in {execution_time:.2f}s)\")\n\n# ===================================================================","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T16:55:32.884604Z","iopub.execute_input":"2025-10-13T16:55:32.885167Z"}},"outputs":[{"name":"stdout","text":"--- Starting Model Benchmarking ---\n\n--- Training LightGBM ---\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 4.217944 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624263\n[LightGBM] [Info] Number of data points in the train set: 60000, number of used features: 2437\n[LightGBM] [Info] Start training from score 2.740904\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"# 3. SUMMARIZE RESULTS AND SELECT THE BEST MODEL\n# ===================================================================\nprint(\"\\n--- Benchmark Summary ---\")\nfor name, result in results.items():\n    print(f\"{name:<10}: {result['score']:.4f} SMAPE | {result['time']:.2f} seconds\")\n\n# Find the best model based on the lowest SMAPE score\nbest_model_name = min(results, key=lambda k: results[k]['score'])\nprint(f\"\\n🏆 Best performing model: {best_model_name} with a score of {results[best_model_name]['score']:.4f}\")\n\n# ===================================================================","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 4. TRAIN THE BEST MODEL ON ALL DATA AND SUBMIT\n# ===================================================================\nprint(f\"\\n--- Retraining the best model ({best_model_name}) on the entire dataset ---\")\n\n# Create a new instance of the best model with the optimal number of estimators\nn_estimators_final = best_iterations[best_model_name]\nif n_estimators_final is None or n_estimators_final == 0: # Handle cases where early stopping wasn't used\n    n_estimators_final = 1000\n\nprint(f\"Using {n_estimators_final} estimators for the final model.\")\n\nif best_model_name == \"LightGBM\":\n    final_model = lgb.LGBMRegressor(random_state=42, n_estimators=n_estimators_final)\n    final_model.fit(X, y, categorical_feature=['brand'])\nelif best_model_name == \"XGBoost\":\n    final_model = xgb.XGBRegressor(random_state=42, n_estimators=n_estimators_final, n_jobs=-1)\n    X_xgb = X.copy()\n    X_xgb['brand'] = X_xgb['brand'].cat.codes\n    final_model.fit(X_xgb, y)\n    X_test_xgb = X_test.copy()\n    X_test_xgb['brand'] = X_test_xgb['brand'].cat.codes\n    X_test = X_test_xgb\nelif best_model_name == \"CatBoost\":\n    final_model = cb.CatBoostRegressor(random_state=42, n_estimators=n_estimators_final, verbose=0)\n    final_model.fit(X, y, cat_features=['brand'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Predict on the test set\nfinal_log_predictions = final_model.predict(X_test)\nfinal_predictions = np.expm1(final_log_predictions)\nfinal_predictions[final_predictions < 0] = 0\n\n# Create and save the submission dataframe\nsubmission_df = pd.DataFrame({'sample_id': test_df['sample_id'], 'price': final_predictions})\nsubmission_df.to_csv('final_submission.csv', index=False)\n\nprint(\"\\nSubmission file 'final_submission.csv' created successfully!\")\nprint(\"This file was generated using the best model from the benchmark.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}