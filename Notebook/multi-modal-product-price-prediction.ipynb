{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T15:51:52.091953Z",
     "iopub.status.busy": "2025-10-13T15:51:52.091666Z",
     "iopub.status.idle": "2025-10-13T15:52:04.387798Z",
     "shell.execute_reply": "2025-10-13T15:52:04.387073Z",
     "shell.execute_reply.started": "2025-10-13T15:51:52.091932Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup Complete. All functions and libraries are loaded.\n"
     ]
    }
   ],
   "source": [
    "# --- Imports ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil # <-- Import shutil for deleting directories\n",
    "from pathlib import Path\n",
    "import urllib.request\n",
    "import multiprocessing\n",
    "from functools import partial\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# --- Model & Feature Extraction Imports ---\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# --- Helper Function: SMAPE Metric ---\n",
    "def smape(y_true, y_pred):\n",
    "    numerator = np.abs(y_pred - y_true)\n",
    "    denominator = (np.abs(y_true) + np.abs(y_pred)) / 2\n",
    "    epsilon = 1e-8\n",
    "    return np.mean(numerator / (denominator + epsilon)) * 100\n",
    "\n",
    "# --- Helper Function: Image Downloader ---\n",
    "def download_image(image_link, savefolder):\n",
    "    if isinstance(image_link, str):\n",
    "        filename = Path(image_link).name\n",
    "        image_save_path = os.path.join(savefolder, filename)\n",
    "        if not os.path.exists(image_save_path):\n",
    "            try:\n",
    "                urllib.request.urlretrieve(image_link, image_save_path)\n",
    "            except Exception as ex:\n",
    "                pass\n",
    "    return\n",
    "\n",
    "def download_images_parallel(image_links, download_folder):\n",
    "    if not os.path.exists(download_folder):\n",
    "        os.makedirs(download_folder)\n",
    "    download_image_partial = partial(download_image, savefolder=download_folder)\n",
    "    with multiprocessing.Pool(processes=multiprocessing.cpu_count()) as pool:\n",
    "        list(tqdm(pool.imap(download_image_partial, image_links), total=len(image_links)))\n",
    "\n",
    "print(\"Setup Complete. All functions and libraries are loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T15:52:04.389636Z",
     "iopub.status.busy": "2025-10-13T15:52:04.389009Z",
     "iopub.status.idle": "2025-10-13T15:52:04.394188Z",
     "shell.execute_reply": "2025-10-13T15:52:04.393360Z",
     "shell.execute_reply.started": "2025-10-13T15:52:04.389614Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence-transformers: 2.7.0\n",
      "transformers: 4.41.2\n",
      "huggingface-hub: 0.23.2\n"
     ]
    }
   ],
   "source": [
    "import sentence_transformers, transformers, huggingface_hub\n",
    "print(\"sentence-transformers:\", sentence_transformers.__version__)\n",
    "print(\"transformers:\", transformers.__version__)\n",
    "print(\"huggingface-hub:\", huggingface_hub.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T15:52:04.395949Z",
     "iopub.status.busy": "2025-10-13T15:52:04.395736Z",
     "iopub.status.idle": "2025-10-13T15:52:04.470363Z",
     "shell.execute_reply": "2025-10-13T15:52:04.469625Z",
     "shell.execute_reply.started": "2025-10-13T15:52:04.395933Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check for GPU availability and set the device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T15:52:04.472075Z",
     "iopub.status.busy": "2025-10-13T15:52:04.471800Z",
     "iopub.status.idle": "2025-10-13T15:52:04.485193Z",
     "shell.execute_reply": "2025-10-13T15:52:04.484517Z",
     "shell.execute_reply.started": "2025-10-13T15:52:04.472056Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#!pip install tensorflow==2.16.1 --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T15:52:04.486017Z",
     "iopub.status.busy": "2025-10-13T15:52:04.485781Z",
     "iopub.status.idle": "2025-10-13T15:52:04.496902Z",
     "shell.execute_reply": "2025-10-13T15:52:04.496120Z",
     "shell.execute_reply.started": "2025-10-13T15:52:04.486002Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#!pip install -U sentence-transformers==2.7.0 transformers==4.41.2 huggingface-hub==0.23.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T15:52:04.660109Z",
     "iopub.status.busy": "2025-10-13T15:52:04.659918Z",
     "iopub.status.idle": "2025-10-13T15:52:07.729917Z",
     "shell.execute_reply": "2025-10-13T15:52:07.729024Z",
     "shell.execute_reply.started": "2025-10-13T15:52:04.660094Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Loading Full Data ---\n",
      "DataFrames loaded successfully.\n",
      "Using 75000 training samples and 75000 testing samples.\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Load Data ---\n",
    "print(\"--- Loading Full Data ---\")\n",
    "train_df = pd.read_csv('/kaggle/input/amazon-ml/train.csv')\n",
    "test_df = pd.read_csv('/kaggle/input/amazon-ml/test.csv')\n",
    "\n",
    "\n",
    "# Apply log transformation to the price\n",
    "train_df['log_price'] = np.log1p(train_df['price'])\n",
    "print(\"DataFrames loaded successfully.\")\n",
    "print(f\"Using {len(train_df)} training samples and {len(test_df)} testing samples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T15:52:07.731510Z",
     "iopub.status.busy": "2025-10-13T15:52:07.731278Z",
     "iopub.status.idle": "2025-10-13T15:52:07.745889Z",
     "shell.execute_reply": "2025-10-13T15:52:07.745127Z",
     "shell.execute_reply.started": "2025-10-13T15:52:07.731492Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_id</th>\n",
       "      <th>catalog_content</th>\n",
       "      <th>image_link</th>\n",
       "      <th>price</th>\n",
       "      <th>log_price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>33127</td>\n",
       "      <td>Item Name: La Victoria Green Taco Sauce Mild, ...</td>\n",
       "      <td>https://m.media-amazon.com/images/I/51mo8htwTH...</td>\n",
       "      <td>4.89</td>\n",
       "      <td>1.773256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>198967</td>\n",
       "      <td>Item Name: Salerno Cookies, The Original Butte...</td>\n",
       "      <td>https://m.media-amazon.com/images/I/71YtriIHAA...</td>\n",
       "      <td>13.12</td>\n",
       "      <td>2.647592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>261251</td>\n",
       "      <td>Item Name: Bear Creek Hearty Soup Bowl, Creamy...</td>\n",
       "      <td>https://m.media-amazon.com/images/I/51+PFEe-w-...</td>\n",
       "      <td>1.97</td>\n",
       "      <td>1.088562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>55858</td>\n",
       "      <td>Item Name: Judee’s Blue Cheese Powder 11.25 oz...</td>\n",
       "      <td>https://m.media-amazon.com/images/I/41mu0HAToD...</td>\n",
       "      <td>30.34</td>\n",
       "      <td>3.444895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>292686</td>\n",
       "      <td>Item Name: kedem Sherry Cooking Wine, 12.7 Oun...</td>\n",
       "      <td>https://m.media-amazon.com/images/I/41sA037+Qv...</td>\n",
       "      <td>66.49</td>\n",
       "      <td>4.211979</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sample_id                                    catalog_content  \\\n",
       "0      33127  Item Name: La Victoria Green Taco Sauce Mild, ...   \n",
       "1     198967  Item Name: Salerno Cookies, The Original Butte...   \n",
       "2     261251  Item Name: Bear Creek Hearty Soup Bowl, Creamy...   \n",
       "3      55858  Item Name: Judee’s Blue Cheese Powder 11.25 oz...   \n",
       "4     292686  Item Name: kedem Sherry Cooking Wine, 12.7 Oun...   \n",
       "\n",
       "                                          image_link  price  log_price  \n",
       "0  https://m.media-amazon.com/images/I/51mo8htwTH...   4.89   1.773256  \n",
       "1  https://m.media-amazon.com/images/I/71YtriIHAA...  13.12   2.647592  \n",
       "2  https://m.media-amazon.com/images/I/51+PFEe-w-...   1.97   1.088562  \n",
       "3  https://m.media-amazon.com/images/I/41mu0HAToD...  30.34   3.444895  \n",
       "4  https://m.media-amazon.com/images/I/41sA037+Qv...  66.49   4.211979  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T15:52:07.747048Z",
     "iopub.status.busy": "2025-10-13T15:52:07.746789Z",
     "iopub.status.idle": "2025-10-13T15:52:13.065131Z",
     "shell.execute_reply": "2025-10-13T15:52:13.064249Z",
     "shell.execute_reply.started": "2025-10-13T15:52:07.747026Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Training image embeddings already exist. Loading from file.\n",
      "Train image embeddings ready. Shape: (75000, 2048)\n"
     ]
    }
   ],
   "source": [
    "# --- 2. Process Training Images ---\n",
    "TRAIN_IMAGE_DIR = 'dataset/images/train'\n",
    "TRAIN_IMG_EMBEDDINGS_FILE = 'train_image_embeddings.npy'\n",
    "\n",
    "if os.path.exists(TRAIN_IMG_EMBEDDINGS_FILE):\n",
    "    print(\"✅ Training image embeddings already exist. Loading from file.\")\n",
    "    train_image_embeddings = np.load(TRAIN_IMG_EMBEDDINGS_FILE)\n",
    "else:\n",
    "    print(f\"--- Downloading {len(train_df)} Training Images ---\")\n",
    "    download_images_parallel(train_df['image_link'].tolist(), TRAIN_IMAGE_DIR)\n",
    "    \n",
    "    print(\"\\n--- Extracting Training Image Features ---\")\n",
    "    image_model = ResNet50(weights='imagenet', include_top=False, pooling='avg')\n",
    "\n",
    "    \n",
    "    \n",
    "    # MODIFIED Code\n",
    "    def extract_image_features_batched(image_paths, batch_size=64):\n",
    "        all_features = []\n",
    "        num_images = len(image_paths)\n",
    "        for i in tqdm(range(0, num_images, batch_size)):\n",
    "            batch_paths = image_paths[i:i + batch_size]\n",
    "            batch_images = []\n",
    "            # Keep track of which images failed to load\n",
    "            valid_indices = []\n",
    "            \n",
    "            for idx, path in enumerate(batch_paths):\n",
    "                try:\n",
    "                    img = image.load_img(path, target_size=(224, 224))\n",
    "                    img_array = image.img_to_array(img)\n",
    "                    batch_images.append(img_array)\n",
    "                    valid_indices.append(True)\n",
    "                except Exception:\n",
    "                    valid_indices.append(False) # Mark this image as failed\n",
    "    \n",
    "            if not batch_images: # If the whole batch failed\n",
    "                all_features.extend([np.zeros(2048) for _ in batch_paths])\n",
    "                continue\n",
    "                \n",
    "            # Preprocess the entire batch at once\n",
    "            batch_images_np = np.array(batch_images)\n",
    "            preprocessed_batch = preprocess_input(batch_images_np)\n",
    "            \n",
    "            # Predict on the entire batch\n",
    "            feature_vectors = image_model.predict(preprocessed_batch, verbose=0)\n",
    "            \n",
    "            # Place results back correctly, filling failures with zeros\n",
    "            features_with_zeros = []\n",
    "            feature_iter = iter(feature_vectors)\n",
    "            for is_valid in valid_indices:\n",
    "                if is_valid:\n",
    "                    features_with_zeros.append(next(feature_iter).flatten())\n",
    "                else:\n",
    "                    features_with_zeros.append(np.zeros(2048))\n",
    "            all_features.extend(features_with_zeros)\n",
    "    \n",
    "        return np.array(all_features)\n",
    "    \n",
    "    # --- Then, call the new function ---\n",
    "    train_image_paths = [os.path.join(TRAIN_IMAGE_DIR, Path(link).name) for link in train_df['image_link']]\n",
    "    train_image_embeddings = extract_image_features_batched(train_image_paths)\n",
    "    # (and do the same for the test images in the next cell)\n",
    "    \n",
    "    print(\"\\n--- Saving Training Image Embeddings ---\")\n",
    "    np.save(TRAIN_IMG_EMBEDDINGS_FILE, train_image_embeddings)\n",
    "    \n",
    "    print(\"\\n--- Deleting Training Images to Free Space ---\")\n",
    "    shutil.rmtree(TRAIN_IMAGE_DIR)\n",
    "    print(\"✅ Training images deleted.\")\n",
    "\n",
    "print(f\"Train image embeddings ready. Shape: {train_image_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T15:52:13.067040Z",
     "iopub.status.busy": "2025-10-13T15:52:13.066788Z",
     "iopub.status.idle": "2025-10-13T15:52:13.515012Z",
     "shell.execute_reply": "2025-10-13T15:52:13.514098Z",
     "shell.execute_reply.started": "2025-10-13T15:52:13.067022Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Test image embeddings already exist. Loading from file.\n",
      "Test image embeddings ready. Shape: (75000, 2048)\n"
     ]
    }
   ],
   "source": [
    "# --- 3. Process Test Images ---\n",
    "TEST_IMAGE_DIR = 'dataset/images/test'\n",
    "TEST_IMG_EMBEDDINGS_FILE = 'test_image_embeddings.npy'\n",
    "\n",
    "if os.path.exists(TEST_IMG_EMBEDDINGS_FILE):\n",
    "    print(\"✅ Test image embeddings already exist. Loading from file.\")\n",
    "    test_image_embeddings = np.load(TEST_IMG_EMBEDDINGS_FILE)\n",
    "else:\n",
    "    print(f\"--- Downloading {len(test_df)} Test Images ---\")\n",
    "    download_images_parallel(test_df['image_link'].tolist(), TEST_IMAGE_DIR)\n",
    "    \n",
    "    print(\"\\n--- Extracting Test Image Features ---\")\n",
    "    # We can reuse the image_model if the previous cell was run in the same session\n",
    "    try:\n",
    "        image_model\n",
    "    except NameError:\n",
    "        image_model = ResNet50(weights='imagenet', include_top=False, pooling='avg')\n",
    "\n",
    "    # Re-using the feature extraction function defined in the previous cell\n",
    "    test_image_paths = [os.path.join(TEST_IMAGE_DIR, Path(link).name) for link in test_df['image_link']]\n",
    "    test_image_embeddings = extract_image_features_batched(test_image_paths)\n",
    "    \n",
    "    print(\"\\n--- Saving Test Image Embeddings ---\")\n",
    "    np.save(TEST_IMG_EMBEDDINGS_FILE, test_image_embeddings)\n",
    "    \n",
    "    print(\"\\n--- Deleting Test Images to Free Space ---\")\n",
    "    shutil.rmtree(TEST_IMAGE_DIR)\n",
    "    print(\"✅ Test images deleted.\")\n",
    "\n",
    "print(f\"Test image embeddings ready. Shape: {test_image_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T15:52:13.516494Z",
     "iopub.status.busy": "2025-10-13T15:52:13.515932Z",
     "iopub.status.idle": "2025-10-13T15:52:13.607467Z",
     "shell.execute_reply": "2025-10-13T15:52:13.606775Z",
     "shell.execute_reply.started": "2025-10-13T15:52:13.516463Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Loading text embeddings from saved files ---\n",
      "✅ Text embeddings loaded successfully.\n",
      "Train text embeddings shape: (75000, 384)\n"
     ]
    }
   ],
   "source": [
    "# --- 4. Text Feature Extraction ---\n",
    "TRAIN_TXT_EMBEDDINGS_FILE = 'train_text_embeddings.npy'\n",
    "TEST_TXT_EMBEDDINGS_FILE = 'test_text_embeddings.npy'\n",
    "\n",
    "if os.path.exists(TRAIN_TXT_EMBEDDINGS_FILE) and os.path.exists(TEST_TXT_EMBEDDINGS_FILE):\n",
    "    print(\"--- Loading text embeddings from saved files ---\")\n",
    "    train_text_embeddings = np.load(TRAIN_TXT_EMBEDDINGS_FILE)\n",
    "    test_text_embeddings = np.load(TEST_TXT_EMBEDDINGS_FILE)\n",
    "    print(\"✅ Text embeddings loaded successfully.\")\n",
    "else:\n",
    "    print(\"--- Generating and saving text embeddings ---\")\n",
    "    # MODIFIED Code\n",
    "    # Explicitly tell the model to use the 'cuda' (GPU) device\n",
    "    text_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2', trust_remote_code=True, device='cuda')\n",
    "    \n",
    "    # Increase the batch size to process more data at once on the GPU\n",
    "    train_text_embeddings = text_model.encode(\n",
    "        train_df['catalog_content'].tolist(),\n",
    "        show_progress_bar=True,\n",
    "        batch_size=256  # Adjust batch size based on GPU memory\n",
    "    )\n",
    "    test_text_embeddings = text_model.encode(\n",
    "        test_df['catalog_content'].tolist(),\n",
    "        show_progress_bar=True,\n",
    "        batch_size=256\n",
    "    )\n",
    "    \n",
    "    np.save(TRAIN_TXT_EMBEDDINGS_FILE, train_text_embeddings)\n",
    "    np.save(TEST_TXT_EMBEDDINGS_FILE, test_text_embeddings)\n",
    "    print(\"✅ Text embeddings generated and saved.\")\n",
    "\n",
    "print(f\"Train text embeddings shape: {train_text_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T15:52:13.608446Z",
     "iopub.status.busy": "2025-10-13T15:52:13.608183Z",
     "iopub.status.idle": "2025-10-13T15:54:41.832925Z",
     "shell.execute_reply": "2025-10-13T15:54:41.832094Z",
     "shell.execute_reply.started": "2025-10-13T15:52:13.608427Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Combining Features and Training Model ---\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 4.652498 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 620160\n",
      "[LightGBM] [Info] Number of data points in the train set: 60000, number of used features: 2432\n",
      "[LightGBM] [Info] Start training from score 2.740904\n",
      "\n",
      "--- Evaluating the new model ---\n",
      "✅ Validation SMAPE Score (Multi-modal): 60.4032\n"
     ]
    }
   ],
   "source": [
    "# --- 5. Combine Features & Train Model ---\n",
    "print(\"\\n--- Combining Features and Training Model ---\")\n",
    "combined_train_features = np.hstack([train_text_embeddings, train_image_embeddings])\n",
    "\n",
    "X = combined_train_features\n",
    "y = train_df['log_price']\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "y_val_original = np.expm1(y_val)\n",
    "\n",
    "# --- You can change model parameters here and rerun ---\n",
    "lgbm = lgb.LGBMRegressor(random_state=42, n_estimators=200, learning_rate=0.05)\n",
    "lgbm.fit(X_train, y_train)\n",
    "\n",
    "# --- Evaluate the new model ---\n",
    "print(\"\\n--- Evaluating the new model ---\")\n",
    "log_val_preds = lgbm.predict(X_val)\n",
    "val_preds = np.expm1(log_val_preds)\n",
    "validation_score = smape(y_val_original, val_preds)\n",
    "print(f\"✅ Validation SMAPE Score (Multi-modal): {validation_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Some Feature Engineering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T15:54:41.835316Z",
     "iopub.status.busy": "2025-10-13T15:54:41.834857Z",
     "iopub.status.idle": "2025-10-13T15:54:41.842591Z",
     "shell.execute_reply": "2025-10-13T15:54:41.841690Z",
     "shell.execute_reply.started": "2025-10-13T15:54:41.835297Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ===================================================================\n",
    "# 1. DEFINE ALL FEATURE ENGINEERING FUNCTIONS\n",
    "# ===================================================================\n",
    "\n",
    "# --- For Quantity (Weight/Volume) ---\n",
    "UNIT_CONVERSIONS = {\n",
    "    # Weight\n",
    "    'kg': 1000, 'kilogram': 1000, 'kgs': 1000,\n",
    "    'g': 1, 'gm': 1, 'gram': 1, 'gms': 1,\n",
    "    'mg': 0.001, 'milligram': 0.001,\n",
    "    'lb': 453.592, 'pound': 453.592, 'lbs': 453.592,\n",
    "    'oz': 28.35, 'ounce': 28.35,\n",
    "    # Volume\n",
    "    'l': 1000, 'liter': 1000, 'liters': 1000,\n",
    "    'ml': 1, 'milliliter': 1,\n",
    "    'floz': 29.5735, 'fluid ounce': 29.5735\n",
    "}\n",
    "\n",
    "def extract_quantity(text):\n",
    "    text = str(text).lower()\n",
    "    pattern = r\"(\\d+\\.?\\d*)\\s?(\" + \"|\".join(UNIT_CONVERSIONS.keys()) + r\")\\b\"\n",
    "    match = re.search(pattern, text)\n",
    "    if match:\n",
    "        value = float(match.group(1))\n",
    "        unit = match.group(2)\n",
    "        return value * UNIT_CONVERSIONS[unit]\n",
    "    return np.nan\n",
    "\n",
    "# --- For Pack Count ---\n",
    "def extract_pack_count(text):\n",
    "    text = str(text).lower()\n",
    "    match = re.search(r\"(?:pack|set)\\s+of\\s+(\\d+)\", text)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    match = re.search(r\"(\\d+)\\s*(?:-|pack|count|ct)\\b\", text)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    return 1 # Default to 1 if no pack info found\n",
    "\n",
    "# --- For Brand Name ---\n",
    "def extract_brand(text):\n",
    "    try:\n",
    "        return str(text).split()[0].lower()\n",
    "    except IndexError:\n",
    "        return \"unknown\"\n",
    "\n",
    "# ==================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T15:54:41.843649Z",
     "iopub.status.busy": "2025-10-13T15:54:41.843379Z",
     "iopub.status.idle": "2025-10-13T15:54:49.149711Z",
     "shell.execute_reply": "2025-10-13T15:54:49.148801Z",
     "shell.execute_reply.started": "2025-10-13T15:54:41.843628Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Feature Engineering ---\n",
      "Processing DataFrame of shape: (75000, 5)\n",
      "Processing DataFrame of shape: (75000, 3)\n",
      "\n",
      "--- Raw Feature Extraction Complete ---\n"
     ]
    }
   ],
   "source": [
    "# 2. APPLY FUNCTIONS TO DATAFRAMES\n",
    "# ===================================================================\n",
    "print(\"--- Starting Feature Engineering ---\")\n",
    "\n",
    "# We'll operate on both DataFrames at once\n",
    "for df in [train_df, test_df]:\n",
    "    print(f\"Processing DataFrame of shape: {df.shape}\")\n",
    "    \n",
    "    # Extract numerical features\n",
    "    df['quantity_std'] = df['catalog_content'].apply(extract_quantity)\n",
    "    df['pack_count'] = df['catalog_content'].apply(extract_pack_count)\n",
    "    df['text_length'] = df['catalog_content'].str.len()\n",
    "    \n",
    "    # Extract keyword flag\n",
    "    df['is_organic'] = df['catalog_content'].str.contains('organic', case=False, regex=False).astype(int)\n",
    "    \n",
    "    # Extract brand name\n",
    "    df['brand'] = df['catalog_content'].apply(extract_brand)\n",
    "\n",
    "print(\"\\n--- Raw Feature Extraction Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T15:54:49.150931Z",
     "iopub.status.busy": "2025-10-13T15:54:49.150619Z",
     "iopub.status.idle": "2025-10-13T15:54:49.162492Z",
     "shell.execute_reply": "2025-10-13T15:54:49.161805Z",
     "shell.execute_reply.started": "2025-10-13T15:54:49.150914Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1458: RuntimeWarning: invalid value encountered in greater\n",
      "  has_large_values = (abs_vals > 1e6).any()\n",
      "/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in less\n",
      "  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n",
      "/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in greater\n",
      "  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_id</th>\n",
       "      <th>catalog_content</th>\n",
       "      <th>image_link</th>\n",
       "      <th>quantity_std</th>\n",
       "      <th>pack_count</th>\n",
       "      <th>text_length</th>\n",
       "      <th>is_organic</th>\n",
       "      <th>brand</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100179</td>\n",
       "      <td>Item Name: Rani 14-Spice Eshamaya's Mango Chut...</td>\n",
       "      <td>https://m.media-amazon.com/images/I/71hoAn78AW...</td>\n",
       "      <td>297.675</td>\n",
       "      <td>14</td>\n",
       "      <td>1274</td>\n",
       "      <td>0</td>\n",
       "      <td>item</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>245611</td>\n",
       "      <td>Item Name: Natural MILK TEA Flavoring extract ...</td>\n",
       "      <td>https://m.media-amazon.com/images/I/61ex8NHCIj...</td>\n",
       "      <td>56.700</td>\n",
       "      <td>1</td>\n",
       "      <td>1720</td>\n",
       "      <td>0</td>\n",
       "      <td>item</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>146263</td>\n",
       "      <td>Item Name: Honey Filled Hard Candy - Bulk Pack...</td>\n",
       "      <td>https://m.media-amazon.com/images/I/61KCM61J8e...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>769</td>\n",
       "      <td>0</td>\n",
       "      <td>item</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>95658</td>\n",
       "      <td>Item Name: Vlasic Snack'mm's Kosher Dill 16 Oz...</td>\n",
       "      <td>https://m.media-amazon.com/images/I/51Ex6uOH7y...</td>\n",
       "      <td>453.600</td>\n",
       "      <td>2</td>\n",
       "      <td>82</td>\n",
       "      <td>0</td>\n",
       "      <td>item</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>36806</td>\n",
       "      <td>Item Name: McCormick Culinary Vanilla Extract,...</td>\n",
       "      <td>https://m.media-amazon.com/images/I/71QYlrOMoS...</td>\n",
       "      <td>946.352</td>\n",
       "      <td>32</td>\n",
       "      <td>1491</td>\n",
       "      <td>0</td>\n",
       "      <td>item</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sample_id                                    catalog_content  \\\n",
       "0     100179  Item Name: Rani 14-Spice Eshamaya's Mango Chut...   \n",
       "1     245611  Item Name: Natural MILK TEA Flavoring extract ...   \n",
       "2     146263  Item Name: Honey Filled Hard Candy - Bulk Pack...   \n",
       "3      95658  Item Name: Vlasic Snack'mm's Kosher Dill 16 Oz...   \n",
       "4      36806  Item Name: McCormick Culinary Vanilla Extract,...   \n",
       "\n",
       "                                          image_link  quantity_std  \\\n",
       "0  https://m.media-amazon.com/images/I/71hoAn78AW...       297.675   \n",
       "1  https://m.media-amazon.com/images/I/61ex8NHCIj...        56.700   \n",
       "2  https://m.media-amazon.com/images/I/61KCM61J8e...           NaN   \n",
       "3  https://m.media-amazon.com/images/I/51Ex6uOH7y...       453.600   \n",
       "4  https://m.media-amazon.com/images/I/71QYlrOMoS...       946.352   \n",
       "\n",
       "   pack_count  text_length  is_organic brand  \n",
       "0          14         1274           0  item  \n",
       "1           1         1720           0  item  \n",
       "2           2          769           0  item  \n",
       "3           2           82           0  item  \n",
       "4          32         1491           0  item  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T15:54:49.163738Z",
     "iopub.status.busy": "2025-10-13T15:54:49.163307Z",
     "iopub.status.idle": "2025-10-13T15:54:49.196883Z",
     "shell.execute_reply": "2025-10-13T15:54:49.196087Z",
     "shell.execute_reply.started": "2025-10-13T15:54:49.163720Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sample_id              0\n",
       "catalog_content        0\n",
       "image_link             0\n",
       "quantity_std       20637\n",
       "pack_count             0\n",
       "text_length            0\n",
       "is_organic             0\n",
       "brand                  0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T15:54:49.198544Z",
     "iopub.status.busy": "2025-10-13T15:54:49.197751Z",
     "iopub.status.idle": "2025-10-13T15:54:49.231523Z",
     "shell.execute_reply": "2025-10-13T15:54:49.230720Z",
     "shell.execute_reply.started": "2025-10-13T15:54:49.198522Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing quantities filled with median value: 283.5\n",
      "Brand column successfully encoded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_28340/3420886458.py:11: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['quantity_std'].fillna(median_quantity, inplace=True)\n",
      "/tmp/ipykernel_28340/3420886458.py:12: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['text_length'].fillna(median_length, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# 3. HANDLE MISSING VALUES & ENCODE CATEGORICALS\n",
    "# ===================================================================\n",
    "\n",
    "# --- Handle Missing Numerical Values ---\n",
    "# Calculate median from the training set ONLY\n",
    "median_quantity = train_df['quantity_std'].median()\n",
    "median_length = train_df['text_length'].median()\n",
    "\n",
    "# Fill NaNs in both train and test sets using the training median\n",
    "for df in [train_df, test_df]:\n",
    "    df['quantity_std'].fillna(median_quantity, inplace=True)\n",
    "    df['text_length'].fillna(median_length, inplace=True)\n",
    "\n",
    "print(f\"Missing quantities filled with median value: {median_quantity}\")\n",
    "\n",
    "# --- Encode Brand as a Categorical Feature ---\n",
    "# Get all unique brands across both datasets to ensure consistency\n",
    "all_brands = pd.concat([train_df['brand'], test_df['brand']]).unique()\n",
    "\n",
    "# Convert the 'brand' column into a pandas Categorical type\n",
    "train_df['brand'] = pd.Categorical(train_df['brand'], categories=all_brands)\n",
    "test_df['brand'] = pd.Categorical(test_df['brand'], categories=all_brands)\n",
    "\n",
    "print(\"Brand column successfully encoded.\")\n",
    "\n",
    "# ==================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T15:54:49.232530Z",
     "iopub.status.busy": "2025-10-13T15:54:49.232307Z",
     "iopub.status.idle": "2025-10-13T15:54:49.253690Z",
     "shell.execute_reply": "2025-10-13T15:54:49.253024Z",
     "shell.execute_reply.started": "2025-10-13T15:54:49.232503Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sample_id          0\n",
       "catalog_content    0\n",
       "image_link         0\n",
       "quantity_std       0\n",
       "pack_count         0\n",
       "text_length        0\n",
       "is_organic         0\n",
       "brand              0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T15:54:49.256661Z",
     "iopub.status.busy": "2025-10-13T15:54:49.256024Z",
     "iopub.status.idle": "2025-10-13T15:54:49.278024Z",
     "shell.execute_reply": "2025-10-13T15:54:49.277471Z",
     "shell.execute_reply.started": "2025-10-13T15:54:49.256642Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Feature Engineering Complete! ---\n",
      "Final engineered features ready for model training. Example:\n",
      "   quantity_std  pack_count  text_length  is_organic brand\n",
      "0      340.2000           6           91           0  item\n",
      "1      226.8000           4          511           0  item\n",
      "2       53.8650           6          328           0  item\n",
      "3      318.9375           1         1318           0  item\n",
      "4      360.0450           1          155           0  item\n",
      "\n",
      "Data types of new features:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 75000 entries, 0 to 74999\n",
      "Data columns (total 5 columns):\n",
      " #   Column        Non-Null Count  Dtype   \n",
      "---  ------        --------------  -----   \n",
      " 0   quantity_std  75000 non-null  float64 \n",
      " 1   pack_count    75000 non-null  int64   \n",
      " 2   text_length   75000 non-null  int64   \n",
      " 3   is_organic    75000 non-null  int64   \n",
      " 4   brand         75000 non-null  category\n",
      "dtypes: category(1), float64(1), int64(3)\n",
      "memory usage: 2.4 MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# 4. PREPARE FINAL FEATURE SET\n",
    "# ===================================================================\n",
    "\n",
    "# Select all the engineered feature columns we want to use\n",
    "engineered_feature_cols = ['quantity_std', 'pack_count', 'text_length', 'is_organic', 'brand']\n",
    "\n",
    "# Create the final feature DataFrames for the model\n",
    "train_engineered_features_df = train_df[engineered_feature_cols]\n",
    "test_engineered_features_df = test_df[engineered_feature_cols]\n",
    "\n",
    "print(\"\\n--- Feature Engineering Complete! ---\")\n",
    "print(\"Final engineered features ready for model training. Example:\")\n",
    "print(train_engineered_features_df.head())\n",
    "print(\"\\nData types of new features:\")\n",
    "print(train_engineered_features_df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T15:54:49.279355Z",
     "iopub.status.busy": "2025-10-13T15:54:49.278729Z",
     "iopub.status.idle": "2025-10-13T15:54:54.208678Z",
     "shell.execute_reply": "2025-10-13T15:54:54.207845Z",
     "shell.execute_reply.started": "2025-10-13T15:54:49.279338Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Combining all feature sets with unique column names ---\n",
      "Final training feature shape: (75000, 2437)\n",
      "Final test feature shape: (75000, 2437)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(\"--- Combining all feature sets with unique column names ---\")\n",
    "\n",
    "# Convert numpy embedding arrays to pandas DataFrames with unique prefixes\n",
    "train_text_df = pd.DataFrame(train_text_embeddings, index=train_df.index, columns=[f'txt_{i}' for i in range(train_text_embeddings.shape[1])])\n",
    "train_image_df = pd.DataFrame(train_image_embeddings, index=train_df.index, columns=[f'img_{i}' for i in range(train_image_embeddings.shape[1])])\n",
    "\n",
    "test_text_df = pd.DataFrame(test_text_embeddings, index=test_df.index, columns=[f'txt_{i}' for i in range(test_text_embeddings.shape[1])])\n",
    "test_image_df = pd.DataFrame(test_image_embeddings, index=test_df.index, columns=[f'img_{i}' for i in range(test_image_embeddings.shape[1])])\n",
    "\n",
    "# Concatenate all features horizontally (axis=1)\n",
    "X = pd.concat([\n",
    "    train_text_df,\n",
    "    train_image_df,\n",
    "    train_engineered_features_df\n",
    "], axis=1)\n",
    "\n",
    "X_test = pd.concat([\n",
    "    test_text_df,\n",
    "    test_image_df,\n",
    "    test_engineered_features_df\n",
    "], axis=1)\n",
    "\n",
    "y = train_df['log_price']\n",
    "\n",
    "print(f\"Final training feature shape: {X.shape}\")\n",
    "print(f\"Final test feature shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T15:54:54.209691Z",
     "iopub.status.busy": "2025-10-13T15:54:54.209460Z",
     "iopub.status.idle": "2025-10-13T15:54:54.215182Z",
     "shell.execute_reply": "2025-10-13T15:54:54.214362Z",
     "shell.execute_reply.started": "2025-10-13T15:54:54.209675Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(75000, 2437)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T15:54:54.216358Z",
     "iopub.status.busy": "2025-10-13T15:54:54.216006Z",
     "iopub.status.idle": "2025-10-13T15:54:55.322211Z",
     "shell.execute_reply": "2025-10-13T15:54:55.321605Z",
     "shell.execute_reply.started": "2025-10-13T15:54:54.216334Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Import all the models we want to test\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import catboost as cb\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# ===================================================================\n",
    "# 1. SETUP THE BENCHMARK\n",
    "# ===================================================================\n",
    "\n",
    "# Split data for validation, which we'll use for all models\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "y_val_original = np.expm1(y_val) # For SMAPE calculation\n",
    "\n",
    "# Define the models to test in a dictionary\n",
    "models = {\n",
    "    \"LightGBM\": lgb.LGBMRegressor(random_state=42, n_estimators=1000),\n",
    "    \"XGBoost\": xgb.XGBRegressor(random_state=42, n_estimators=1000, n_jobs=-1, early_stopping_rounds=100),\n",
    "    \"CatBoost\": cb.CatBoostRegressor(random_state=42, n_estimators=1000, verbose=0, early_stopping_rounds=100)\n",
    "}\n",
    "\n",
    "results = {}\n",
    "best_iterations = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T15:54:55.323660Z",
     "iopub.status.busy": "2025-10-13T15:54:55.322957Z",
     "iopub.status.idle": "2025-10-13T16:15:31.772656Z",
     "shell.execute_reply": "2025-10-13T16:15:31.771838Z",
     "shell.execute_reply.started": "2025-10-13T15:54:55.323630Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Model Benchmarking ---\n",
      "\n",
      "--- Training LightGBM ---\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 4.164428 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 620869\n",
      "[LightGBM] [Info] Number of data points in the train set: 60000, number of used features: 2436\n",
      "[LightGBM] [Info] Start training from score 2.740904\n",
      "✅ LightGBM Validation SMAPE: 55.2404 (trained in 469.42s)\n",
      "\n",
      "--- Training XGBoost ---\n",
      "[0]\tvalidation_0-rmse:0.90113\n",
      "[1]\tvalidation_0-rmse:0.87000\n",
      "[2]\tvalidation_0-rmse:0.84868\n",
      "[3]\tvalidation_0-rmse:0.83370\n",
      "[4]\tvalidation_0-rmse:0.82329\n",
      "[5]\tvalidation_0-rmse:0.81651\n",
      "[6]\tvalidation_0-rmse:0.80998\n",
      "[7]\tvalidation_0-rmse:0.80513\n",
      "[8]\tvalidation_0-rmse:0.80084\n",
      "[9]\tvalidation_0-rmse:0.79622\n",
      "[10]\tvalidation_0-rmse:0.79350\n",
      "[11]\tvalidation_0-rmse:0.79031\n",
      "[12]\tvalidation_0-rmse:0.78817\n",
      "[13]\tvalidation_0-rmse:0.78586\n",
      "[14]\tvalidation_0-rmse:0.78390\n",
      "[15]\tvalidation_0-rmse:0.78244\n",
      "[16]\tvalidation_0-rmse:0.78091\n",
      "[17]\tvalidation_0-rmse:0.77939\n",
      "[18]\tvalidation_0-rmse:0.77805\n",
      "[19]\tvalidation_0-rmse:0.77765\n",
      "[20]\tvalidation_0-rmse:0.77613\n",
      "[21]\tvalidation_0-rmse:0.77563\n",
      "[22]\tvalidation_0-rmse:0.77420\n",
      "[23]\tvalidation_0-rmse:0.77287\n",
      "[24]\tvalidation_0-rmse:0.77256\n",
      "[25]\tvalidation_0-rmse:0.77247\n",
      "[26]\tvalidation_0-rmse:0.77167\n",
      "[27]\tvalidation_0-rmse:0.77133\n",
      "[28]\tvalidation_0-rmse:0.77066\n",
      "[29]\tvalidation_0-rmse:0.77031\n",
      "[30]\tvalidation_0-rmse:0.76982\n",
      "[31]\tvalidation_0-rmse:0.76967\n",
      "[32]\tvalidation_0-rmse:0.76925\n",
      "[33]\tvalidation_0-rmse:0.76840\n",
      "[34]\tvalidation_0-rmse:0.76808\n",
      "[35]\tvalidation_0-rmse:0.76771\n",
      "[36]\tvalidation_0-rmse:0.76750\n",
      "[37]\tvalidation_0-rmse:0.76714\n",
      "[38]\tvalidation_0-rmse:0.76714\n",
      "[39]\tvalidation_0-rmse:0.76717\n",
      "[40]\tvalidation_0-rmse:0.76734\n",
      "[41]\tvalidation_0-rmse:0.76696\n",
      "[42]\tvalidation_0-rmse:0.76698\n",
      "[43]\tvalidation_0-rmse:0.76628\n",
      "[44]\tvalidation_0-rmse:0.76618\n",
      "[45]\tvalidation_0-rmse:0.76599\n",
      "[46]\tvalidation_0-rmse:0.76594\n",
      "[47]\tvalidation_0-rmse:0.76572\n",
      "[48]\tvalidation_0-rmse:0.76572\n",
      "[49]\tvalidation_0-rmse:0.76562\n",
      "[50]\tvalidation_0-rmse:0.76561\n",
      "[51]\tvalidation_0-rmse:0.76521\n",
      "[52]\tvalidation_0-rmse:0.76486\n",
      "[53]\tvalidation_0-rmse:0.76497\n",
      "[54]\tvalidation_0-rmse:0.76466\n",
      "[55]\tvalidation_0-rmse:0.76488\n",
      "[56]\tvalidation_0-rmse:0.76459\n",
      "[57]\tvalidation_0-rmse:0.76469\n",
      "[58]\tvalidation_0-rmse:0.76428\n",
      "[59]\tvalidation_0-rmse:0.76443\n",
      "[60]\tvalidation_0-rmse:0.76466\n",
      "[61]\tvalidation_0-rmse:0.76475\n",
      "[62]\tvalidation_0-rmse:0.76469\n",
      "[63]\tvalidation_0-rmse:0.76465\n",
      "[64]\tvalidation_0-rmse:0.76462\n",
      "[65]\tvalidation_0-rmse:0.76433\n",
      "[66]\tvalidation_0-rmse:0.76415\n",
      "[67]\tvalidation_0-rmse:0.76422\n",
      "[68]\tvalidation_0-rmse:0.76407\n",
      "[69]\tvalidation_0-rmse:0.76400\n",
      "[70]\tvalidation_0-rmse:0.76371\n",
      "[71]\tvalidation_0-rmse:0.76387\n",
      "[72]\tvalidation_0-rmse:0.76359\n",
      "[73]\tvalidation_0-rmse:0.76297\n",
      "[74]\tvalidation_0-rmse:0.76287\n",
      "[75]\tvalidation_0-rmse:0.76284\n",
      "[76]\tvalidation_0-rmse:0.76270\n",
      "[77]\tvalidation_0-rmse:0.76250\n",
      "[78]\tvalidation_0-rmse:0.76247\n",
      "[79]\tvalidation_0-rmse:0.76230\n",
      "[80]\tvalidation_0-rmse:0.76210\n",
      "[81]\tvalidation_0-rmse:0.76199\n",
      "[82]\tvalidation_0-rmse:0.76214\n",
      "[83]\tvalidation_0-rmse:0.76219\n",
      "[84]\tvalidation_0-rmse:0.76221\n",
      "[85]\tvalidation_0-rmse:0.76198\n",
      "[86]\tvalidation_0-rmse:0.76189\n",
      "[87]\tvalidation_0-rmse:0.76186\n",
      "[88]\tvalidation_0-rmse:0.76176\n",
      "[89]\tvalidation_0-rmse:0.76197\n",
      "[90]\tvalidation_0-rmse:0.76221\n",
      "[91]\tvalidation_0-rmse:0.76245\n",
      "[92]\tvalidation_0-rmse:0.76240\n",
      "[93]\tvalidation_0-rmse:0.76237\n",
      "[94]\tvalidation_0-rmse:0.76246\n",
      "[95]\tvalidation_0-rmse:0.76247\n",
      "[96]\tvalidation_0-rmse:0.76246\n",
      "[97]\tvalidation_0-rmse:0.76249\n",
      "[98]\tvalidation_0-rmse:0.76263\n",
      "[99]\tvalidation_0-rmse:0.76240\n",
      "[100]\tvalidation_0-rmse:0.76253\n",
      "[101]\tvalidation_0-rmse:0.76235\n",
      "[102]\tvalidation_0-rmse:0.76257\n",
      "[103]\tvalidation_0-rmse:0.76230\n",
      "[104]\tvalidation_0-rmse:0.76213\n",
      "[105]\tvalidation_0-rmse:0.76207\n",
      "[106]\tvalidation_0-rmse:0.76215\n",
      "[107]\tvalidation_0-rmse:0.76225\n",
      "[108]\tvalidation_0-rmse:0.76238\n",
      "[109]\tvalidation_0-rmse:0.76260\n",
      "[110]\tvalidation_0-rmse:0.76264\n",
      "[111]\tvalidation_0-rmse:0.76260\n",
      "[112]\tvalidation_0-rmse:0.76263\n",
      "[113]\tvalidation_0-rmse:0.76287\n",
      "[114]\tvalidation_0-rmse:0.76310\n",
      "[115]\tvalidation_0-rmse:0.76314\n",
      "[116]\tvalidation_0-rmse:0.76328\n",
      "[117]\tvalidation_0-rmse:0.76343\n",
      "[118]\tvalidation_0-rmse:0.76322\n",
      "[119]\tvalidation_0-rmse:0.76315\n",
      "[120]\tvalidation_0-rmse:0.76326\n",
      "[121]\tvalidation_0-rmse:0.76338\n",
      "[122]\tvalidation_0-rmse:0.76344\n",
      "[123]\tvalidation_0-rmse:0.76351\n",
      "[124]\tvalidation_0-rmse:0.76334\n",
      "[125]\tvalidation_0-rmse:0.76338\n",
      "[126]\tvalidation_0-rmse:0.76338\n",
      "[127]\tvalidation_0-rmse:0.76337\n",
      "[128]\tvalidation_0-rmse:0.76354\n",
      "[129]\tvalidation_0-rmse:0.76340\n",
      "[130]\tvalidation_0-rmse:0.76336\n",
      "[131]\tvalidation_0-rmse:0.76325\n",
      "[132]\tvalidation_0-rmse:0.76342\n",
      "[133]\tvalidation_0-rmse:0.76330\n",
      "[134]\tvalidation_0-rmse:0.76334\n",
      "[135]\tvalidation_0-rmse:0.76329\n",
      "[136]\tvalidation_0-rmse:0.76332\n",
      "[137]\tvalidation_0-rmse:0.76342\n",
      "[138]\tvalidation_0-rmse:0.76314\n",
      "[139]\tvalidation_0-rmse:0.76315\n",
      "[140]\tvalidation_0-rmse:0.76329\n",
      "[141]\tvalidation_0-rmse:0.76326\n",
      "[142]\tvalidation_0-rmse:0.76330\n",
      "[143]\tvalidation_0-rmse:0.76329\n",
      "[144]\tvalidation_0-rmse:0.76333\n",
      "[145]\tvalidation_0-rmse:0.76322\n",
      "[146]\tvalidation_0-rmse:0.76315\n",
      "[147]\tvalidation_0-rmse:0.76327\n",
      "[148]\tvalidation_0-rmse:0.76342\n",
      "[149]\tvalidation_0-rmse:0.76349\n",
      "[150]\tvalidation_0-rmse:0.76359\n",
      "[151]\tvalidation_0-rmse:0.76362\n",
      "[152]\tvalidation_0-rmse:0.76357\n",
      "[153]\tvalidation_0-rmse:0.76364\n",
      "[154]\tvalidation_0-rmse:0.76351\n",
      "[155]\tvalidation_0-rmse:0.76370\n",
      "[156]\tvalidation_0-rmse:0.76361\n",
      "[157]\tvalidation_0-rmse:0.76346\n",
      "[158]\tvalidation_0-rmse:0.76333\n",
      "[159]\tvalidation_0-rmse:0.76319\n",
      "[160]\tvalidation_0-rmse:0.76324\n",
      "[161]\tvalidation_0-rmse:0.76319\n",
      "[162]\tvalidation_0-rmse:0.76310\n",
      "[163]\tvalidation_0-rmse:0.76299\n",
      "[164]\tvalidation_0-rmse:0.76300\n",
      "[165]\tvalidation_0-rmse:0.76290\n",
      "[166]\tvalidation_0-rmse:0.76287\n",
      "[167]\tvalidation_0-rmse:0.76278\n",
      "[168]\tvalidation_0-rmse:0.76301\n",
      "[169]\tvalidation_0-rmse:0.76335\n",
      "[170]\tvalidation_0-rmse:0.76336\n",
      "[171]\tvalidation_0-rmse:0.76343\n",
      "[172]\tvalidation_0-rmse:0.76342\n",
      "[173]\tvalidation_0-rmse:0.76355\n",
      "[174]\tvalidation_0-rmse:0.76368\n",
      "[175]\tvalidation_0-rmse:0.76352\n",
      "[176]\tvalidation_0-rmse:0.76337\n",
      "[177]\tvalidation_0-rmse:0.76330\n",
      "[178]\tvalidation_0-rmse:0.76345\n",
      "[179]\tvalidation_0-rmse:0.76318\n",
      "[180]\tvalidation_0-rmse:0.76324\n",
      "[181]\tvalidation_0-rmse:0.76330\n",
      "[182]\tvalidation_0-rmse:0.76336\n",
      "[183]\tvalidation_0-rmse:0.76340\n",
      "[184]\tvalidation_0-rmse:0.76355\n",
      "[185]\tvalidation_0-rmse:0.76367\n",
      "[186]\tvalidation_0-rmse:0.76356\n",
      "[187]\tvalidation_0-rmse:0.76380\n",
      "[188]\tvalidation_0-rmse:0.76394\n",
      "✅ XGBoost Validation SMAPE: 58.4899 (trained in 298.68s)\n",
      "\n",
      "--- Training CatBoost ---\n",
      "✅ CatBoost Validation SMAPE: 56.1273 (trained in 468.35s)\n"
     ]
    }
   ],
   "source": [
    "# ===================================================================\n",
    "# 2. RUN THE BENCHMARKING LOOP\n",
    "# ===================================================================\n",
    "print(\"--- Starting Model Benchmarking ---\")\n",
    "\n",
    "for name, model in models.items():\n",
    "    start_time = time.time()\n",
    "    print(f\"\\n--- Training {name} ---\")\n",
    "\n",
    "    # Train each model with its specific parameters\n",
    "    if name == \"LightGBM\":\n",
    "        model.fit(X_train, y_train, eval_set=[(X_val, y_val)],\n",
    "                  callbacks=[lgb.early_stopping(100, verbose=False)],\n",
    "                  categorical_feature=['brand'])\n",
    "        best_iterations[name] = model.best_iteration_\n",
    "    elif name == \"XGBoost\":\n",
    "        # XGBoost needs category columns to be of 'category' dtype, which we already did\n",
    "        X_train_xgb = X_train.copy()\n",
    "        X_val_xgb = X_val.copy()\n",
    "        X_train_xgb['brand'] = X_train_xgb['brand'].cat.codes\n",
    "        X_val_xgb['brand'] = X_val_xgb['brand'].cat.codes\n",
    "        model.fit(X_train_xgb, y_train, eval_set=[(X_val_xgb, y_val)])\n",
    "        best_iterations[name] = model.best_iteration\n",
    "    elif name == \"CatBoost\":\n",
    "        model.fit(X_train, y_train, eval_set=[(X_val, y_val)],\n",
    "                  cat_features=['brand'])\n",
    "        best_iterations[name] = model.best_iteration_\n",
    "    \n",
    "    # Make predictions and evaluate\n",
    "    log_val_preds = model.predict(X_val)\n",
    "    val_preds = np.expm1(log_val_preds)\n",
    "    score = smape(y_val_original, val_preds)\n",
    "    \n",
    "    # Store results\n",
    "    execution_time = time.time() - start_time\n",
    "    results[name] = {'score': score, 'time': execution_time}\n",
    "    print(f\"✅ {name} Validation SMAPE: {score:.4f} (trained in {execution_time:.2f}s)\")\n",
    "\n",
    "# ==================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T16:15:31.774039Z",
     "iopub.status.busy": "2025-10-13T16:15:31.773500Z",
     "iopub.status.idle": "2025-10-13T16:15:31.779239Z",
     "shell.execute_reply": "2025-10-13T16:15:31.778523Z",
     "shell.execute_reply.started": "2025-10-13T16:15:31.774019Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Benchmark Summary ---\n",
      "LightGBM  : 55.2404 SMAPE | 469.42 seconds\n",
      "XGBoost   : 58.4899 SMAPE | 298.68 seconds\n",
      "CatBoost  : 56.1273 SMAPE | 468.35 seconds\n",
      "\n",
      "🏆 Best performing model: LightGBM with a score of 55.2404\n"
     ]
    }
   ],
   "source": [
    "# 3. SUMMARIZE RESULTS AND SELECT THE BEST MODEL\n",
    "# ===================================================================\n",
    "print(\"\\n--- Benchmark Summary ---\")\n",
    "for name, result in results.items():\n",
    "    print(f\"{name:<10}: {result['score']:.4f} SMAPE | {result['time']:.2f} seconds\")\n",
    "\n",
    "# Find the best model based on the lowest SMAPE score\n",
    "best_model_name = min(results, key=lambda k: results[k]['score'])\n",
    "print(f\"\\n🏆 Best performing model: {best_model_name} with a score of {results[best_model_name]['score']:.4f}\")\n",
    "\n",
    "# ==================================================================="
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8465669,
     "sourceId": 13348797,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31154,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
